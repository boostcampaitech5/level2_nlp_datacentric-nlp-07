{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data-Centric NLP 대회: 주제 분류 프로젝트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tokenization_kobert import KoBertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 456\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(BASE_DIR, '../data')\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, '../output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'monologg/kobert'\n",
    "tokenizer = KoBertTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "int2label = [\"IT과학\", \"경제\", \"사회\", \"생활문화\", \"세계\", \"스포츠\", \"정치\"]\n",
    "label2int = {\"IT과학\": 0, \"경제\":1, \"사회\":2, \"생활문화\":3, \"세계\":4, \"스포츠\":5, \"정치\":6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30042</th>\n",
       "      <td>ynat-v1_train_30042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>https://news.naver.com/main/read.nhn?mode=LS2D...</td>\n",
       "      <td>2018.12.21. 오후 4:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        ID text  target   \n",
       "30042  ynat-v1_train_30042  NaN       5  \\\n",
       "\n",
       "                                                     url                 date  \n",
       "30042  https://news.naver.com/main/read.nhn?mode=LS2D...  2018.12.21. 오후 4:14  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ID, text, target, url, date]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/g2p_clean_train.csv\")\n",
    "display(data[data[\"text\"].isnull() == True].head())\n",
    "data = data.drop(data[data[\"text\"].isnull() == True].index)\n",
    "display(data[data[\"text\"].isnull() == True].head())\n",
    "# data = data.drop(data.index[30042]) # nan 제거\n",
    "# data.loc[30042, \"text\"] = \"함께 아리랑\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                                      ynat-v1_train_30042\n",
       "text                                                 함께 아리랑\n",
       "target                                                    5\n",
       "url       https://news.naver.com/main/read.nhn?mode=LS2D...\n",
       "date                                    2018.12.21. 오후 4:14\n",
       "Name: 30042, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[30042]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        input_texts = data['text']\n",
    "        targets = data['target']\n",
    "        self.inputs = []; self.labels = []\n",
    "        self.ids = []\n",
    "        for text, label in zip(input_texts, targets):\n",
    "            tokenized_input = tokenizer(text, max_length=40,padding='max_length', truncation=True, return_tensors='pt')\n",
    "            self.inputs.append(tokenized_input)\n",
    "            self.labels.append(torch.tensor(label))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.inputs[idx]['input_ids'].squeeze(0),  \n",
    "            'attention_mask': self.inputs[idx]['attention_mask'].squeeze(0),\n",
    "            'labels': self.labels[idx].squeeze(0)\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset has 7 classes.\n",
      "Classes: {0, 1, 2, 3, 4, 5, 6}\n"
     ]
    }
   ],
   "source": [
    "raw_texts, labels = data[\"text\"].values, data[\"target\"].values\n",
    "num_classes = len(set(labels))\n",
    "print(f\"This dataset has {num_classes} classes.\")\n",
    "print(f\"Classes: {set(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train = BERTDataset(dataset_train, tokenizer)\n",
    "# data_valid = BERTDataset(dataset_valid, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = evaluate.load('f1')\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return f1.compute(predictions=predictions, references=labels, average='macro')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    logging_strategy='no',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='no',\n",
    "    logging_steps=100,\n",
    "    eval_steps=500,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    learning_rate= 2e-05,\n",
    "    adam_beta1 = 0.9,\n",
    "    adam_beta2 = 0.999,\n",
    "    adam_epsilon=1e-08,\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type='linear',\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=2,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model='eval_f1',\n",
    "    greater_is_better=True,\n",
    "    seed=SEED,\n",
    "    report_to='none',\n",
    "    # fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['내년부터 국가RD 평가 때 논문건수는 반영 않는다', '어버이날 맑다가 흐려져 ... 남부지방 옅은 황사']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[[2,1]][\"text\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "pred_probs = []\n",
    "text_embeddings = []\n",
    "indices = []\n",
    "N_SPLITS = 10\n",
    "skfold = StratifiedKFold(n_splits=N_SPLITS) # 5 -> valid 0.2\n",
    "cnt = 0\n",
    "for train_index, valid_index in skfold.split(raw_texts,labels):\n",
    "    cnt+=1\n",
    "    print(f\"*** kfold: {cnt}/{N_SPLITS} ***\")\n",
    "    data_train = BERTDataset(data.iloc[train_index], tokenizer)\n",
    "    data_valid = BERTDataset(data.iloc[valid_index], tokenizer)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=7).to(DEVICE)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=data_train,\n",
    "        eval_dataset=data_valid,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    \n",
    "    model.eval()\n",
    "    BATCH_SIZE=2048\n",
    "    valid_text = data.iloc[valid_index][\"text\"].values.tolist()\n",
    "    indices.append(valid_index)\n",
    "    for batch in tqdm(DataLoader(valid_text, batch_size=BATCH_SIZE)):\n",
    "        inputs = tokenizer(batch, max_length=40, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_hidden_states=True)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.nn.Softmax(dim=1)(logits).cpu().numpy()\n",
    "            pred_probs.append(probs)\n",
    "            emb = outputs.hidden_states[-1][:,0,:].cpu().numpy()\n",
    "            text_embeddings.append(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([   0,    1,    2, ..., 4975, 4979, 4984]),\n",
       " array([ 4017,  4019,  4023, ..., 10028, 10034, 10035]),\n",
       " array([ 8199,  8204,  8206, ..., 15188, 15193, 15204]),\n",
       " array([12121, 12130, 12140, ..., 20089, 20092, 20093]),\n",
       " array([16389, 16391, 16397, ..., 25072, 25077, 25082]),\n",
       " array([20587, 20599, 20601, ..., 29694, 29695, 29703]),\n",
       " array([24437, 24462, 24469, ..., 35330, 35341, 35347]),\n",
       " array([28638, 28649, 28658, ..., 39037, 39044, 39048]),\n",
       " array([32736, 32749, 32757, ..., 42495, 42498, 42508]),\n",
       " array([37589, 37611, 37668, ..., 45674, 45675, 45676])]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indices = np.concatenate(indices)\n",
    "pred_probs = np.concatenate(pred_probs)\n",
    "text_embeddings = np.concatenate(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45677, 7) (45677, 768) (45677,)\n"
     ]
    }
   ],
   "source": [
    "# print(pred_probs.shape, indices.shape)\n",
    "print(pred_probs.shape, text_embeddings.shape, indices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## get text_embeddings from pre_trained model (not finetuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/monologg/kobert/resolve/main/config.json from cache at /opt/ml/.cache/huggingface/transformers/31dc8da633439f22ed80bede01f337996bc709eb8429f86f2b24e2103558b039.89a06cdfd16840fd89cc5c2493ef63cd0b6068e85f70ac988a3673e2722cab2e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.8.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 8002\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/monologg/kobert/resolve/main/pytorch_model.bin from cache at /opt/ml/.cache/huggingface/transformers/9525d6f96682baa1f21538ea58d36263fe16a46345dd9637e3e28a4df2f9380f.ebe6e13ff204bebbffd4764cda3d5a97dc690a9c4110bde6d909ddc3ed5c4585\n",
      "All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "All the weights of BertModel were initialized from the model checkpoint at monologg/kobert.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from kobert_transformers import get_kobert_model\n",
    "transformer = get_kobert_model().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:26<00:00,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45677, 768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE=2048\n",
    "transformer.eval()\n",
    "text_embeddings = []\n",
    "for batch_indices in tqdm(DataLoader(indices, batch_size=BATCH_SIZE)):\n",
    "    batch = data.iloc[batch_indices][\"text\"].values.tolist()\n",
    "    with torch.no_grad():\n",
    "        input = tokenizer(batch, max_length=40, padding=True, return_tensors=\"pt\").to(DEVICE)\n",
    "        output = transformer(**input).pooler_output.cpu().numpy()\n",
    "    text_embeddings.append(output)\n",
    "    \n",
    "text_embeddings = np.concatenate(text_embeddings)\n",
    "print(text_embeddings.shape)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleanlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding label issues ...\n",
      "Finding outlier issues ...\n",
      "Fitting OOD estimator based on provided features ...\n",
      "Finding near_duplicate issues ...\n",
      "Audit complete. 7630 issues found in the dataset.\n"
     ]
    }
   ],
   "source": [
    "from cleanlab import Datalab\n",
    "data_dict = {\"texts\": data.iloc[indices][\"text\"], \"labels\": data.iloc[indices][\"target\"]}\n",
    "lab = Datalab(data_dict, label_name=\"labels\")\n",
    "lab.find_issues(pred_probs=pred_probs, features=text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< 0.01 1245\n",
      "< 0.02 1777\n"
     ]
    }
   ],
   "source": [
    "label_issues = lab.get_issues(\"label\")\n",
    "res_df = pd.DataFrame(\n",
    "{\n",
    "    \"text\": raw_texts,\n",
    "    \"target\": labels,\n",
    "    \"suggested\": label_issues[\"predicted_label\"],\n",
    "    \"given_label\": [int2label[x] for x in labels],\n",
    "    \"suggested_label\": label_issues[\"predicted_label\"].apply(lambda x: int2label[x]),\n",
    "    \"label_score\": label_issues[\"label_score\"],\n",
    "    \"error\": label_issues[\"is_label_issue\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_df.to_csv(\"cleanlab_result_finetune2.csv\") # cv=10, text_embedding from pretrained (기존에는 각자 다른 fine_tuned 모델에서), loc->iloc 오류 해결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_df = pd.read_csv(\"../cleanlab_result_finetune2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## error_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< 0.01 1245\n",
      "< 0.02 1764\n"
     ]
    }
   ],
   "source": [
    "error_df = res_df[res_df[\"error\"] == True].sort_values(\"label_score\")\n",
    "print(\"< 0.01\",len(error_df[error_df[\"label_score\"]  < 0.01]))\n",
    "print(\"< 0.02\", len(error_df[error_df[\"label_score\"]  < 0.02]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "error_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "error_df[error_df[\"label_score\"] < 0.02].head(1000)#.groupby(\"suggested_label\")[\"text\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = error_df[error_df[\"label_score\"]  < 0.02].index\n",
    "to_change = error_df.loc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### for wandb setting\n",
    "# os.environ['WANDB_DISABLED'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_valid,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
